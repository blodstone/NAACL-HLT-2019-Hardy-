%
% File naacl2019.tex
%
%% Based on the style files for ACL 2018 and NAACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{latexsym}
%\usepackage[draft]{hyperref}
\usepackage{url}
\hbadness=99999
%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\shashi[1]{{\textcolor{blue}{#1}}}

%\title{Towards Standardized Manual Evaluation for Document Summarization}
% Towards Standardized Manual Evaluation for Single Document Automatic Summarization
%\title{Towards Manual Evaluation for Single Document Summarization}
\title{Highlight Based Evaluation of Single Document Summarization}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Automatic summarization research has made substantial progress thanks to novel methods and datasets. Manual evaluation approaches so far either ignore content and focus on fluency, or require expert annotators but nevertheless suffer from low inter-annotator agreement due to the complexity of the task. In the few cases where the contents of the summary are evaluated, the evaluation is biased due to using a single reference summary, which results in different summaries of equal quality being rated according to their similarity to the reference. In this paper, we propose a Highlight-bAsed Evaluation of Single document Summarization (HArnESS). Our proposal assesses summaries against the original document, facilitated through manually highlighted salient content which can be reused in future studies. Furthermore it does not require expert annotators, avoids reference bias and provides absolute instead of ranked evaluation of systems.

%The summarization community could benefit from better well-specified summary evaluation methods. 
%The widely used ROUGE is not accurate due to reference bias caused by  the evaluation against a single reference summary \citep{Louis2013}. Manual evaluations characterized by reading the document and ranking system summaries require expert annotators and suffer from a lack of consensus. 
%In this paper, we present \textsc{SEaMLESS}, a \textbf{S}tandardiz\textbf{E}d \textbf{M}anua\textbf{L} \textbf{E}valuation for \textbf{S}ingle document \textbf{S}ummarization. Our novel framework accesses summaries against the manually annotated informative content in the document. Our annotation is guided by the simple WH questions to determine the informative content in the document. \textsc{SEaMLESS} does not require expert annotators, resolves the reference bias issue and provides a mean of doing an interpretable human evaluation. 


%A well-specified summary evaluation is important for automatic summarization community for ranking different systems. However, the community hasn't reached a consensus on manual evaluation practice which lead to fragmented community and many repeated system evaluation run that is laborious and costly. In this paper, we present Summ-Eval, a manual evaluation toolkit and practice for single document summarization which: (1) provides a means of getting comparable and reusable results, (2) provides evaluation criteria and validation scheme, (3) resolves the reference bias, and (4) is accessible for non-expert evaluators 

\end{abstract}
% \citep{Sandhaus2008, Hermann2015, Shashi2018}
\section{Introduction}

Research in automatic summarization has made headway over the years with single document summarization as the front-runner due to the availability of large datasets. Single document summarization methods \citep{See2017, Kryscinski2018, Shashi2018} aim to produce the highest quality of summary indicated by its capability to capture all the important content of the document while still maintain sentence fluency and clarity. Hence, having %a well-specified summary 
an evaluation that can be consistently applied to assess all aspects of a good summary is important for the community as it provides means to measure progress. %rank different systems.  

% AV: ROUGE is the most popular consistently used, but it is automatic not manual and we know that it can be misleading. Furthermore, recently proposed datasets have a single reference per document, which renders ROUGE less accurate.
ROUGE \citep{Lin2004} for automatic evaluation has seen a wide adoption, but recent releases of newswire dataset such as NY Times \citep{Sandhaus2008}, CNN/Daily Mail \citep{Hermann2015}, and XSum \citep{Shashi2018} are characterized by only having a single reference summary per document which makes ROUGE becomes less accurate due to reference bias \citep{Louis2013}. 

As such, to get a reliable assessment a manual evaluation is needed. However, there is still no consensus on manual evaluation practice yet, unlike other fields like machine translation which have adopted practice and toolkit for manual evaluation like Appraise \citep{Federmann2012}. %AV: which are used in WMT since....
Consequently, many researches report manual evaluation results that are often non-reusable for subsequent researches due to the lack of transparency (e.g. evaluation prompts and setup), and the inherent nature of the evaluation process (e.g. paired comparison and best-worst scaling). This lead to fragmented and many repeated system evaluation run that is laborious and costly. Other prevailing issues are differing views on evaluation criteria and validation scheme, reference bias, and access to expert evaluators (necessary for Pyramid method \citep{Nenkova2004}). 

To show how different each of the manual evaluation, we look into recent practices in a single document summarization field. We divided all these approaches into three categories: ranking scheme, subject of comparison, and method. 

Based on ranking scheme there are three approaches: rating scale \citep{Likert1932} used by \citet{Kryscinski2018}, paired comparison \citep{Thurstone1994} used by \citet{Fan2018, Celikyilmaz2018}; and best-worst scaling \citep{Woodworth1991} used by \citet{Shashi2018}. Then based on subject of comparison, there are head-to-head comparison between system and reference summary \citep{Celikyilmaz2018} and ground truth comparison using the document directly \citep{Shashi2018, Kryscinski2018}. Finally there are also different method in evaluating, for example question-answering method \citep{Clarke2010, Shashi2018}. In addition to that, several researches \citep{Nallapati2016a, See2017, Gehrmann2018} did not employ manual evaluation but instead opted to do qualitative analysis directly on the system summary.

In this paper, we present a toolkit and practice for manual evaluation in single document summarization, \textit{Summ-Eval}: a manual evaluation toolkit and practice which: (1) provides a means of getting comparable and reusable results, (2) provide evaluation criteria and validation scheme, (3) resolves the reference bias, and (4) is accessible for non-expert evaluators.

% \shashi{How is it different from Pyramid: annotates generated summaries, expert annotators?}

%\section{Related Works}
%Test

%\section*{Acknowledgments}
%The acknowledgments should go immediately before the references.  Do not number the acknowledgments section. Do not include this section when submitting your paper for review.

\bibliography{naaclhlt2019}
\bibliographystyle{acl_natbib}

\appendix


\end{document}
