%
% File naacl2019.tex
%
%% Based on the style files for ACL 2018 and NAACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}
\hbadness=99999
%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Towards Standardized Manual Evaluation for Single Document Automatic Summarization}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Having a good summary evaluation approach is important for automatic summarization field, however, in automatic summarization community there is still no consensus in manual evaluation practice. This difficult situation is coupled with the release of recent large newswire dataset for single document summarization that are characterized by only having a single reference summary per document raise an issue of reference bias for common practice in automatic evaluation such as ROUGE. In this paper, we present Summ-Eval, a manual evaluation toolkit and practice for single document summarization which: (1) provides a means of getting comparable and reusable results, (2) provides evaluation criteria and validation scheme, (3) resolves the reference bias, and (4) is accessible for non-expert evaluators 
\end{abstract}
% \citep{Sandhaus2008, Hermann2015, Shashi2018}
\section{Introduction}

Research in automatic summarization has made headway over the years with single document summarization as the front-runner due to the availability of large datasets. Many single document summarization researches \citep{See2017, Kryscinski2018, Shashi2018} are aiming to produce the highest quality of summary indicated by its capability to capture all the important content of the document while still maintain sentence fluency and clarity. Hence, having a good summary evaluation approach is important for the field as it provides a means to rank different systems.  

ROUGE \citep{Lin2004} for automatic evaluation has seen a wide adoption, but recent releases of newswire dataset such as NY Times \citep{Sandhaus2008}, CNN/Daily Mail \citep{Hermann2015}, and XSum \citep{Shashi2018} are characterized by only having a single reference summary per document which makes ROUGE becomes less accurate due to reference bias \citep{Louis2013}. As such, to get a reliable assessment a manual evaluation is needed. However, there is still no consensus in manual evaluation practice yet in automatic summarization field, whereas the machine translation have already adopted practice and toolkit for manual evaluation like Appraise \citep{Federmann2012}. Consequently, many researches report manual evaluation results that are often non-reusable for subsequent researches due to the lack of transparency (e.g. evaluation prompts and setup), and the inherent nature of the evaluation process (e.g. paired comparison and best-worst scaling). This cause many researches have to keep repeating evaluating the same systems which is a laborious and costly task. Other prevailing issues are differing views on evaluation criteria and validation scheme, reference bias, and access to expert evaluators. 

To show how different each of the manual evaluation, we look into recent practices in a single document summarization field. We divided all these approaches into three category: ranking scheme, subject of comparison, and method. 

Based on ranking scheme there are three approaches: rating scale \citep{Likert1932} used by \citet{Kryscinski2018}, paired comparison \citep{Thurstone1994} used by \citet{Fan2018, Celikyilmaz2018}; and best-worst scaling \citep{Woodworth1991} used by \citet{Shashi2018}. Then based on subject of comparison, there are head-to-head comparison between system and reference summary \citep{Celikyilmaz2018} and ground truth comparison using the document directly \citep{Shashi2018, Kryscinski2018}. Finally there are also different method in evaluating, for example question-answering method \citep{Clarke2010, Shashi2018}. In addition to that, some researches \citep{Nallapati2016a, See2017, Gehrmann2018} did not employ manual evaluation but instead opt to do qualitative analysis directly on the system summary.

To resolve all the aforementioned issues, we present a toolkit as well as a standardized practice for manual evaluation in single document summarization, \textit{Summ-Eval}: a manual evaluation toolkit and practice which: (1) provides a means of getting comparable and reusable results, (2) provide evaluation criteria and validation scheme, (3) resolves the reference bias, and (4) is accessible for non-expert evaluators.

\section{Related Works}
Test

\section*{Acknowledgments}

The acknowledgments should go immediately before the references.  Do
not number the acknowledgments section. Do not include this section
when submitting your paper for review.

\bibliography{naaclhlt2019}
\bibliographystyle{acl_natbib}

\appendix


\end{document}
