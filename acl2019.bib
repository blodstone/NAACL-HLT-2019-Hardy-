Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop
@article{sokal1995biometry,
  title={Biometry. 3rd ed},
  city={New York},
  publisher = {WH Freeman and Company},
  author={Sokal, RR and Rohlf, FJ},
  year={1995}
}

@book{everitt2006cambridge,
  title={The Cambridge dictionary of statistics},
  author={Everitt, Brian S},
  year={2006},
  publisher={Cambridge University Press}
}

@inproceedings{dang2005overview,
  title={Overview of {DUC} 2005},
  author={Dang, Hoa Trang},
  year = {2005}
}
@article{Guo2018a,
author = 	"Guo, Han
		and Pasunuru, Ramakanth
		and Bansal, Mohit",
  title = 	"Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation",
  booktitle = 	"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"687--697",
  location = 	"Melbourne, Australia",
  url = 	"http://aclweb.org/anthology/P18-1064"
}

@article{Cao2018a,
author = {Cao, Ziqiang and Li, Wenjie and Li, Sujian and Wei, Furu},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao et al. - 2018 - Retrieve, Rerank and Rewrite Soft Template Based Neural Summarization(3).pdf:pdf},
journal = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
mendeley-groups = {Report and Paper/ACL 2019},
pages = {152--161},
title = {{Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization}},
url = {https://aclanthology.info/papers/P18-1015/p18-1015},
volume = {1},
year = {2018}
}

@inproceedings{Nenkova:2005:ATS,
 author = {Nenkova, Ani},
 title = {Automatic Text Summarization of Newswire: Lessons Learned from the Document Understanding Conference},
 booktitle = {Proceedings of the 20th National Conference on Artificial Intelligence - Volume 3},
 year = {2005},
 location = {Pittsburgh, Pennsylvania},
 pages = {1436--1441},
 numpages = {6},
} 

@inproceedings{narayan-sidenet18,
 author = {Shashi Narayan and Ronald Cardenas and Nikos
                  Papasarantopoulos and Shay B. Cohen and Mirella
                  Lapata and Jiangsheng Yu and Yi Chang},
 title = {Document Modeling with External Attention for Sentence Extraction},
 booktitle = {Proceedings of the 56st Annual Meeting of the
                  Association for Computational Linguistics},
 pages = {2020--2030},
 address = {Melbourne, Australia},
 year = {2018},
}

@inproceedings{Narayan2018,
address = {Stroudsburg, PA, USA},
author = {Narayan, Shashi and Cohen, Shay B. and Lapata, Mirella},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
pages = {1747--1759},
publisher = {Association for Computational Linguistics},
title = {{Ranking Sentences for Extractive Summarization with Reinforcement Learning}},
url = {http://aclweb.org/anthology/N18-1158},
volume = {1},
year = {2018}
}

@inproceedings{Sakaue2018a,
abstract = {Submodular maximization with the greedy algorithm has been studied as an effective approach to extractive summarization. This approach is known to have three advan-tages: its applicability to many useful sub-modular objective functions, the efficiency of the greedy algorithm, and the provable performance guarantee. However, when it comes to compressive summarization, we are currently missing a counterpart of the extractive method based on submodular-ity. In this paper, we propose a fast greedy method for compressive summarization. Our method is applicable to any mono-tone submodular objective function, includ-ing many functions well-suited for docu-ment summarization. We provide an ap-proximation guarantee of our greedy algo-rithm. Experiments show that our method is about 100 to 400 times faster than an existing method based on integer-linear-programming (ILP) formulations and that our method empirically achieves more than 95{\%}-approximation.},
author = {Sakaue, Shinsaku and Hirao, Tsutomu and Nishino, Masaaki and Nagata, Masaaki},
doi = {10.18653/v1/n18-1157},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sakaue et al. - Unknown - Provable Fast Greedy Compressive Summarization with Any Monotone Submodular Function.pdf:pdf},
mendeley-groups = {Report and Paper/ACL 2019},
pages = {1737--1746},
title = {{Provable Fast Greedy Compressive Summarization with Any Monotone Submodular Function}},
url = {http://aclweb.org/anthology/N18-1157},
year = {2018}
}

@article{Pasunuru2018a,
abstract = {Abstractive text summarization is the task of compressing and rewriting a long document into a short summary while maintaining saliency, directed logical entailment, and non-redundancy. In this work, we address these three important aspects of a good summary via a reinforcement learning approach with two novel reward functions: ROUGESal and Entail, on top of a coverage-based baseline. The ROUGESal reward modifies the ROUGE metric by up-weighting the salient phrases/words detected via a keyphrase classifier. The Entail reward gives high (length-normalized) scores to logically-entailed summaries using an entailment classifier. Further, we show superior performance improvement when these rewards are combined with traditional metric (ROUGE) based rewards, via our novel and effective multi-reward approach of optimizing multiple rewards simultaneously in alternate mini-batches. Our method achieves the new state-of-the-art results (including human evaluation) on the CNN/Daily Mail dataset as well as strong improvements in a test-only transfer setup on DUC-2002.},
archivePrefix = {arXiv},
arxivId = {1804.06451},
author = {Pasunuru, Ramakanth and Bansal, Mohit},
doi = {10.18653/v1/N18-2102},
eprint = {1804.06451},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pasunuru, Bansal - Unknown - Multi-Reward Reinforced Summarization with Saliency and Entailment.pdf:pdf},
mendeley-groups = {Report and Paper/ACL 2019},
pages = {646--653},
title = {{Multi-Reward Reinforced Summarization with Saliency and Entailment}},
url = {http://aclweb.org/anthology/N18-2102 http://arxiv.org/abs/1804.06451},
year = {2018}
}

@article{Perrin2017,
abstract = {In neural abstractive summarization, the conventional sequence-to-sequence (seq2seq) model often suffers from repetition and semantic irrelevance. To tackle the problem, we propose a global encoding framework, which controls the information flow from the encoder to the decoder based on the global information of the source context. It consists of a convolutional gated unit to perform global encoding to improve the representations of the source-side information. Evaluations on the LCSTS and the English Gigaword both demonstrate that our model outperforms the baseline models, and the analysis shows that our model is capable of reducing repetition.},
archivePrefix = {arXiv},
arxivId = {1805.03989},
author = {Perrin, Sean and Roos, Carlijn De and Oord, Saskia Van Der and Zijlstra, Bonne and Lucassen, Sacha and Perrin, Sean},
doi = {10.1111/jcpp.12768},
eprint = {1805.03989},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - Unknown - Global Encoding for Abstractive Summarization.pdf:pdf},
mendeley-groups = {Report and Paper/ACL 2019},
number = {June},
pages = {163--169},
title = {{Global Encoding for Abstractive Summarization}},
url = {https://www.github.},
year = {2017}
}

@inproceedings{Krishna2018a,
abstract = {Summarizing a document requires identifying the important parts of the document with an objective of providing a quick overview to a reader. However, a long article can span sev-eral topics and a single summary cannot do justice to all the topics. Further, the interests of readers can vary and the notion of impor-tance can change across them. Existing sum-marization algorithms generate a single sum-mary and are not capable of generating mul-tiple summaries tuned to the interests of the readers. In this paper, we propose an attention based RNN framework to generate multiple summaries of a single document tuned to dif-ferent topics of interest. Our method outper-forms existing baselines and our results sug-gest that the attention of generative networks can be successfully biased to look at sentences relevant to a topic and effectively used to gen-erate topic-tuned summaries.},
author = {Krishna, Kundan and Srinivasan, Balaji Vasan},
doi = {10.18653/v1/n18-1153},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krishna, Research, Srinivasan - Unknown - Generating topic-oriented summaries using neural attention.pdf:pdf},
mendeley-groups = {Report and Paper/ACL 2019},
pages = {1697--1705},
title = {{Generating Topic-Oriented Summaries Using Neural Attention}},
url = {http://aclweb.org/anthology/N18-1153},
year = {2018}
}

@article{Jadhav2018a,
abstract = {We present a new neural sequence-to-sequence model for extractive summa-rization called SWAP-NET (Sentences and Words from Alternating Pointer Networks). Extractive summaries comprising a salient subset of input sentences, often also contain important key words. Guided by this principle, we design SWAP-NET that models the interaction of key words and salient sentences using a new two-level pointer network based architecture. SWAP-NET identifies both salient sentences and key words in an input document , and then combines them to form the extractive summary. Experiments on large scale benchmark corpora demonstrate the efficacy of SWAP-NET that outperforms state-of-the-art extractive summarizers.},
author = {Jadhav, Aishwarya},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jadhav, Rajan - Unknown - Extractive Summarization with SWAP-NET Sentences and Words from Alternating Pointer Networks.pdf:pdf},
journal = {Acl 2018},
mendeley-groups = {Report and Paper/ACL 2019},
pages = {1--10},
title = {{Extractive Summarization with SWAP-NET : Sentences and Words from Alternating Pointer Networks}},
url = {http://aclweb.org/anthology/P18-1014},
year = {2018}
}

@inproceedings{Yang2018a,
abstract = {We present a robust approach for detecting intrinsic sentence importance in news, by training on two corpora of document-summary pairs. When used for single-document summarization, our approach, combined with the "beginning of docu-ment" heuristic, outperforms a state-of-the-art summarizer and the beginning-of-article baseline in both automatic and manual evaluations. These results represent an important advance because in the absence of cross-document repetition, single document summarizers for news have not been able to consistently outperform the strong beginning-of-article baseline.},
address = {Valencia, Spain},
author = {Yang, Yinfei and {Sheng Bao}, Forrest and Nenkova, Ani},
booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Sheng Bao, Nenkova - Unknown - Detecting (Un)Important Content for Single-Document News Summarization.pdf:pdf},
mendeley-groups = {Report and Paper/ACL 2019},
pages = {707--712},
publisher = {the Association for Computational Linguistics},
title = {{Detecting (Un)Important Content for Single-Document News Summarization}},
url = {https://catalog.ldc.upenn.edu/},
volume = {2},
year = {2017}
}

@inproceedings{Kedzie2018,
abstract = {We carry out experiments with deep learning models of summarization across the domains of news, personal stories, meetings, and medical articles in order to understand how content selection is performed. We find that many sophisticated features of state of the art extractive summarizers do not improve performance over simpler models. These results suggest that it is easier to create a summarizer for a new domain than previous work suggests and bring into question the benefit of deep learning models for summarization for those domains that do have massive datasets (i.e., news). At the same time, they suggest important questions for new research in summarization; namely, new forms of sentence representations or external knowledge sources are needed that are better suited to the sumarization task.},
address = {Brussels, Belgium},
archivePrefix = {arXiv},
arxivId = {1810.12343v1},
author = {Kedzie, Chris and McKeown, Kathleen and {Daum{\'{e}} III}, Hal},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
eprint = {1810.12343v1},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kedzie, McKeown, Daum{\'{e}} III - Unknown - Content Selection in Deep Learning Models of Summarization.pdf:pdf},
mendeley-groups = {Experiments/Elly: Human Evaluation/Summarization,Report and Paper/ACL 2019},
pages = {1818----1828},
publisher = {Association for Computational Linguistics},
title = {{Content Selection in Deep Learning Models of Summarization}},
url = {https://github.com/kedz/nnsum/},
year = {2018}
}

@techreport{Liao2018a,
abstract = {Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to multi-document summarization have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation (AMR), a semantic representation of natural language grounded in linguistic theory, as a form of content representation. Our approach condenses source documents to a set of summary graphs following the AMR formalism. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The framework is fully data-driven and flexible. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research.},
archivePrefix = {arXiv},
arxivId = {1806.05655},
author = {Liao, Kexin and Lebanoff, Logan and Liu, Fei},
eprint = {1806.05655},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liao, Lebanoff, Liu - Unknown - Abstract Meaning Representation for Multi-Document Summarization(2).pdf:pdf},
issn = {2156-6976 (Print)},
mendeley-groups = {Report and Paper/ACL 2019},
pages = {1178--1190},
pmid = {21968515},
title = {{Abstract Meaning Representation for Multi-Document Summarization}},
url = {http://arxiv.org/abs/1806.05655},
year = {2018}
}
@article{Amplayo2018a,
abstract = {A major proportion of a text summary includes important entities found in the original text. These entities build up the topic of the summary. Moreover, they hold commonsense information once they are linked to a knowledge base. Based on these observations, this paper investigates the usage of linked entities to guide the decoder of a neural text summarizer to generate concise and better summaries. To this end, we leverage on an off-the-shelf entity linking system (ELS) to extract linked entities and propose Entity2Topic (E2T), a module easily attachable to a sequence-to-sequence model that transforms a list of entities into a vector representation of the topic of the summary. Current available ELS's are still not sufficiently effective, possibly introducing unresolved ambiguities and irrelevant entities. We resolve the imperfections of the ELS by (a) encoding entities with selective disambiguation, and (b) pooling entity vectors using firm attention. By applying E2T to a simple sequence-to-sequence model with attention mechanism as base model, we see significant improvements of the performance in the Gigaword (sentence to title) and CNN (long document to multi-sentence highlights) summarization datasets by at least 2 ROUGE points.},
archivePrefix = {arXiv},
arxivId = {1806.05504},
author = {Amplayo, Reinald Kim and Lim, Seonjae and Hwang, Seung-Won},
doi = {10.18653/v1/N18-1064},
eprint = {1806.05504},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Amplayo, Lim, Hwang - Unknown - Entity Commonsense Representation for Neural Abstractive Summarization.pdf:pdf},
mendeley-groups = {Report and Paper/ACL 2019},
pages = {697--707},
title = {{Entity Commonsense Representation for Neural Abstractive Summarization}},
url = {https://github.com/idio/wiki2vec http://arxiv.org/abs/1806.05504},
year = {2018}
}


@InProceedings{newsroom_N181065,
  author = 	"Grusky, Max
		and Naaman, Mor
		and Artzi, Yoav",
  title = 	"Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies",
  booktitle = 	"Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"708--719",
  location = 	"New Orleans, Louisiana",
  url = 	"http://aclweb.org/anthology/N18-1065"
}

@inproceedings{novikova2017we,
  title={Why We Need New Evaluation Metrics for {NLG}},
  author={Novikova, Jekaterina and Du{\v{s}}ek, Ond{\v{r}}ej and Curry, Amanda Cercas and Rieser, Verena},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={2241--2252},
  year={2017}
}

@inproceedings{Cohan2018a,
abstract = {Neural abstractive summarization models have led to promising results in summarizing relatively short documents. We propose the first model for abstractive summarization of single, longer-form documents (e.g., research papers). Our approach consists of a new hierarchical encoder that models the discourse structure of a document, and an attentive discourse-aware decoder to generate the summary. Empirical results on two large-scale datasets of scientific papers show that our model significantly out-performs state-of-the-art models.},
author = {Cohan, Arman and Dernoncourt, Franck and Kim, Dernoncourt Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohan, Dernoncourt Doo Soon Kim Trung Bui Seokhwan Kim Walter Chang Nazli Goharian - Unknown - A Discourse-Aware Attention Model for Abs.pdf:pdf},
mendeley-groups = {Report and Paper/ACL 2019},
pages = {615--621},
title = {{A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents}},
url = {https://github.com/acohan/long-summarization},
year = {2018}
}

@InProceedings{gehrmann2018bottom,
  author    = {Gehrmann, Sebastian  and  Deng, Yuntian  and  Rush, Alexander},
  title     = {Bottom-Up Abstractive Summarization},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  year      = {2018},
  address   = {Brussels, Belgium},
  pages     = {4098--4109},
}

@InProceedings{schluter:2017:EACLshort,
  author    = {Schluter, Natalie},
  title     = {The limits of automatic summarisation according to ROUGE},
  booktitle = {Proceedings of the 15th Conference of the European
                  Chapter of the Association for Computational
                  Linguistics: Short Papers},
  year      = {2017},
  address   = {Valencia, Spain},
  pages     = {41--45},
}
@article{Chen2018a,
author = {Chen, Yen-Chun and Bansal, Mohit},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Bansal - 2018 - Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting(2).pdf:pdf},
journal = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
mendeley-groups = {Report and Paper/ACL 2019},
pages = {675--686},
title = {{Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting}},
url = {https://aclanthology.info/papers/P18-1063/p18-1063},
volume = {1},
year = {2018}
}

@inproceedings{Li2018c,
abstract = {Neural network models, based on the at-tentional encoder-decoder model, have good capability in abstractive text summarization. However, these models are hard to be con-trolled in the process of generation, which leads to a lack of key information. We propose a guiding generation model that combines the extractive method and the abstractive method. Firstly, we obtain keywords from the text by a extractive model. Then, we introduce a Key Information Guide Network (KIGN), which encodes the keywords to the key information representation, to guide the process of gener-ation. In addition, we use a prediction-guide mechanism, which can obtain the long-term value for future decoding, to further guide the summary generation. We evaluate our model on the CNN/Daily Mail dataset. The exper-imental results show that our model leads to significant improvements.},
author = {Li, Chenliang and Xu, Weiran and Li, Si and Gao, Sheng},
doi = {10.18653/v1/n18-2009},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - Unknown - Guiding Generation for Abstractive Text Summarization based on Key Information Guide Network.pdf:pdf},
mendeley-groups = {Report and Paper/ACL 2019},
pages = {55--60},
title = {{Guiding Generation for Abstractive Text Summarization Based on Key Information Guide Network}},
url = {http://aclweb.org/anthology/N18-2009},
year = {2018}
}
@article{Peyrard2018a,
abstract = {Supervised summarization systems usually rely on supervision at the sentence or n-gram level provided by automatic metrics like ROUGE, which act as noisy proxies for human judgments. In this work, we learn a summary-level scoring function $\theta$ including human judgments as supervision and automatically generated data as regularization. We extract summaries with a genetic algorithm using $\theta$ as a fitness function. We observe strong and promising performances across datasets in both automatic and manual evaluation.},
author = {Peyrard, Maxime and Gurevych, Iryna},
doi = {10.18653/v1/N18-2103},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peyrard, Gurevych - Unknown - Objective Function Learning to Match Human Judgements for Optimization-Based Summarization.pdf:pdf},
mendeley-groups = {Report and Paper/ACL 2019},
pages = {654--660},
title = {{Objective Function Learning to Match Human Judgements for Optimization-Based Summarization}},
url = {http://tac.nist.gov/2008/},
year = {2018}
}
}
@article{Song2018a,
abstract = {Seq2seq learning has produced promising results on summarization. However, in many cases, system summaries still struggle to keep the meaning of the original intact. They may miss out important words or relations that play critical roles in the syntactic structure of source sentences. In this paper, we present structure-infused copy mechanisms to facilitate copying important words and relations from the source sentence to summary sentence. The approach naturally combines source dependency structure with the copy mechanism of an abstractive sentence summarizer. Experimental results demonstrate the effectiveness of incorporating source-side syntactic information in the system, and our proposed approach compares favorably to state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1806.05658},
author = {Song, Kaiqiang and Zhao, Lin and Liu, Fei},
doi = {10.1007/978-3-319-31307-8_20},
eprint = {1806.05658},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Song, Zhao, Liu - Unknown - Structure-Infused Copy Mechanisms for Abstractive Summarization.pdf:pdf},
isbn = {9783319313061},
issn = {21945357},
mendeley-groups = {Report and Paper/ACL 2019},
pages = {1717--1729},
title = {{Structure-Infused Copy Mechanisms for Abstractive Summarization}},
url = {https://github.com/KaiQiangSong/struct{\_}infused{\_}summ http://arxiv.org/abs/1806.05658},
year = {2018}
}

@InProceedings{bansal2018fastabstractive,
  author = 	"Chen, Yen-Chun
		and Bansal, Mohit",
  title = 	"Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting",
  booktitle = 	"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"675--686",
  location = 	"Melbourne, Australia",
  url = 	"http://aclweb.org/anthology/P18-1063"
}

@InProceedings{Pasunuru-multireward18,
  author = 	"Pasunuru, Ramakanth
		and Bansal, Mohit",
  title = 	"Multi-Reward Reinforced Summarization with Saliency and Entailment",
  booktitle = 	"Proceedings of the 2018 Conference of the North American Chapter of the      Association for Computational Linguistics: Human Language Technologies,      Volume 2 (Short Papers)    ",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"646--653",
  location = 	"New Orleans, Louisiana",
  doi = 	"10.18653/v1/N18-2102",
  url = 	"http://aclweb.org/anthology/N18-2102"
}

@inproceedings{li2018guiding,
  title={Guiding Generation for Abstractive Text Summarization Based on Key Information Guide Network},
  author={Li, Chenliang and Xu, Weiran and Li, Si and Gao, Sheng},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
  volume={2},
  pages={55--60},
  year={2018}
}

@InProceedings{zhang2018neural,
  author    = {Zhang, Xingxing  and  Lapata, Mirella  and  Wei, Furu  and  Zhou, Ming},
  title     = {Neural Latent Extractive Document Summarization},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  year      = {2018},
  address   = {Brussels, Belgium},
  pages     = {779--784},
}

@article{Thurstone1994,
author = {Thurstone, L. L.},
file = {:home/acp16hh/Temp Mendeley/thurstone94law.pdf:pdf},
journal = {Psychological review},
number = {2},
pages = {255--270},
title = {{A Law of Comparative Judgment}},
volume = {101},
year = {1994}
}
@inproceedings{Paulus2018,
abstract = {Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. However, for longer documents and summaries, these models often include repetitive and incoherent phrases. We introduce a neural network model with intra-attention and a new training method. This method combines standard supervised word prediction and reinforcement learning (RL). Models trained only with the former often exhibit "exposure bias" -- they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, a 5.7 absolute points improvement over previous state-of-the-art models. It also performs well as the first abstractive model on the New York Times corpus. Human evaluation also shows that our model produces higher quality summaries.},
archivePrefix = {arXiv},
arxivId = {1705.04304},
author = {Paulus, Romain and Xiong, Caiming and Socher, Richard},
booktitle = {Proceedings of the 6th International Conference on Learning Representations},
eprint = {1705.04304},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paulus, Xiong, Socher - 2018 - A Deep Reinforced Model for Abstractive Summarization(2).pdf:pdf},
title = {{A Deep Reinforced Model for Abstractive Summarization}},
url = {http://arxiv.org/abs/1705.04304},
year = {2018}
}
@article{Robertson1946,
author = {Robertson, D. W.},
file = {:home/acp16hh/Temp Mendeley/4172741.pdf:pdf},
journal = {Studies in Philology},
number = {1},
pages = {6--14},
title = {{A Note on the Classical Origin of " Circumstances " in the Medieval Confessional}},
volume = {43},
year = {1946}
}
@misc{Sandhaus2008,
address = {Philadelphia},
author = {Sandhaus, Evan},
publisher = {Linguistic Data Consortium},
title = {{The New York Times Annotated Corpus}},
year = {2008}
}
@inproceedings{See2017,
abstract = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
archivePrefix = {arXiv},
arxivId = {1704.04368},
author = {See, Abigail and Liu, Peter J. and Manning, Christopher D.},
booktitle = {Proceedings of the 55th Annual Meeting of the ACL},
doi = {10.18653/v1/P17-1099},
eprint = {1704.04368},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/See, Liu, Manning - 2017 - Get To The Point Summarization with Pointer-Generator Networks(2).pdf:pdf},
isbn = {9781945626753},
title = {{Get To The Point: Summarization with Pointer-Generator Networks}},
url = {http://arxiv.org/abs/1704.04368},
year = {2017}
}
@inproceedings{Fan2018,
abstract = {Current models for document summarization disregard user preferences such as the desired length, style, the entities that the user might be interested in, or how much of the document the user has already read. We present a neural summarization model with a simple but effective mechanism to enable users to specify these high level attributes in order to control the shape of the final summaries to better suit their needs. With user input, our system can produce high quality summaries that follow user preferences. Without user input, we set the control variables automatically. On the full text CNN-Dailymail dataset, we outperform state of the art abstractive systems (both in terms of F1-ROUGE1 40.38 vs. 39.53 and human evaluation).},
archivePrefix = {arXiv},
arxivId = {1711.05217},
author = {Fan, Angela and Grangier, David and Auli, Michael},
booktitle = {ACL 2018 Workshop on Neural Machine Translation and Generation},
eprint = {1711.05217},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fan, Grangier, Auli - Unknown - Controllable Abstractive Summarization.pdf:pdf},
title = {{Controllable Abstractive Summarization}},
url = {https://arxiv.org/pdf/1711.05217.pdf http://arxiv.org/abs/1711.05217},
year = {2018}
}
@article{Lin2004,
abstract = {ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to auto- matically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of over- lapping units such as n-gram, word sequences, and word pairs between the computer-generated sum- mary to be evaluated and the ideal summaries cre- ated by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summariza- tion evaluation package and their evaluatio ns. Three of them have been used in the Document Under- standing$\backslash$tConference$\backslash$t(DUC)$\backslash$t2004,$\backslash$ta$\backslash$tlarge -scale summarization evaluation sponsored by NIST.},
author = {Lin, Chin-Yew},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin - 2004 - Rouge A package for automatic evaluation of summaries.pdf:pdf},
issn = {00036951},
journal = {Proceedings of the workshop on Text Summarization Branches Out, Post-Conference Workshop of ACL 2004},
keywords = {2004,a package for automatic,barcelona,evaluation of summaries,post-conference workshop of acl,proceedings of workshop on,r ouge,spain,text summarization branches out},
number = {1},
pages = {25--26},
title = {{Rouge: A package for automatic evaluation of summaries}},
url = {papers2://publication/uuid/5DDA0BB8-E59F-44C1-88E6-2AD316DAEF85},
year = {2004}
}
@article{Louis2013,
archivePrefix = {arXiv},
arxivId = {1604.07370},
author = {Louis, Annie and Nenkova, Ani},
doi = {10.1162/COLI},
eprint = {1604.07370},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Louis, Nenkova - 2013 - Automatically Assessing Machine Summary Content Without a Gold Standard.pdf:pdf},
isbn = {9781608459858},
issn = {01272713},
journal = {Computational Linguistics},
number = {2},
pages = {267--300},
pmid = {22251136},
title = {{Automatically Assessing Machine Summary Content Without a Gold Standard}},
url = {http://www.seas.upenn.edu/},
volume = {39},
year = {2013}
}
@article{Nallapati2016a,
abstract = {In this work, we cast abstractive text summarization as a sequence-to-sequence problem and employ the framework of Attentional Encoder-Decoder Recurrent Neural Networks to this problem, outperforming state-of-the art model of Rush et. al. (2015) on two different corpora. We also move beyond the basic architecture, and propose several novel models to address important problems in summarization including modeling key-words, capturing the hierarchy of sentence-to-word structure and addressing the problem of words that are key to a document, but rare elsewhere. Our work shows that many of our proposed solutions contribute to further improvement in performance. In addition, we propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.},
archivePrefix = {arXiv},
arxivId = {1602.06023},
author = {Nallapati, Ramesh and Zhou, Bowen and dos Santos, Cicero Nogueira and Gulcehre, Caglar and Xiang, Bing},
eprint = {1602.06023},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nallapati et al. - 2016 - Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond.pdf:pdf},
journal = {Proceedings of CoNLL},
pages = {280--290},
title = {{Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond}},
url = {http://arxiv.org/abs/1602.06023},
year = {2016}
}
@article{Bojar2011,
author = {Bojar, Ond{\v{r}}ej and Ercegov{\v{c}}evi{\'{c}}, Milo{\v{s}} and Popel, Martin and Zaidan, Omar},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bojar et al. - 2011 - A Grain of Salt for the WMT Manual Evaluation(2).pdf:pdf},
isbn = {978-1-937284-12-1},
journal = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
pages = {1--11},
title = {{A Grain of Salt for the WMT Manual Evaluation}},
url = {http://www.aclweb.org/anthology/W11-2101},
year = {2011}
}
@article{Josep1971,
author = {Fleiss, Joseph L.},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - full-text(3).pdf:pdf},
journal = {Psychological Bulletin},
number = {5},
pages = {378--382},
title = {{Measuring Nominal Scale Agreement Among Many Raters}},
volume = {76},
year = {1971}
}
@article{Federmann2012,
abstract = {We describe a focused effort to investigate the performance of phrase-based,human evaluation of machine translation output achieving a high annotatoragreement. We define phrase-based evaluation and describe the implementation ofAppraise, a toolkit that supports the manual evaluation of machine translationresults. Phrase ranking can be done using either a fine-grained six-way scoringscheme that allows to differentiate between "much better" and "slightlybetter", or a reduced subset of ranking choices. Afterwards we discuss kappavalues for both scoring models from several experiments conducted with humanannotators. Our results show that phrase-based evaluation can be used for fastevaluation obtaining significant agreement among annotators. The granularity ofranking choices should, however, not be too fine-grained as this seems toconfuse annotators and thus reduces the overall agreement. The work reported inthis paper confirms previous work in the field and illustrates that the usageof human evaluation in machine translation should be reconsidered. The Appraisetoolkit is available as open-source and can be downloaded from the author'swebsite.},
author = {Federmann, Christian},
doi = {10.2478/v10108-012-0006-9},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Federmann - 2012 - Appraise an Open-Source Toolkit for Manual Evaluation of MT Output(2).pdf:pdf},
isbn = {2-9517408-6-7},
issn = {1804-0462},
journal = {The Prague Bulletin of Mathematical Linguistics},
keywords = {applications,evaluation,machine translation},
number = {-1},
pages = {130--134},
title = {{Appraise: an Open-Source Toolkit for Manual Evaluation of MT Output}},
url = {http://www.degruyter.com/view/j/pralin.2012.98.issue--1/v10108-012-0006-9/v10108-012-0006-9.xml},
volume = {98},
year = {2012}
}
@article{Bojar2017,
author = {Bojar, Ond{\v{r}}ej and Chatterjee, Rajen and Federmann, Christian and Graham, Yvette and Haddow, Barry and Huang, Shujian and Huck, Matthias and Koehn, Philipp and Liu, Qun and Logacheva, Varvara and Monz, Christof and Negri, Matteo and Post, Matt and Rubino, Raphael and Specia, Lucia and Turchi, Marco},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bojar et al. - 2017 - Findings of the 2017 Conference on Machine Translation (WMT17)(2).pdf:pdf},
journal = {Wmt-2017},
title = {{Findings of the 2017 Conference on Machine Translation (WMT17)}},
year = {2017}
}
@techreport{Li2018b,
abstract = {In this paper, we investigate the sentence summarization task that produces a summary from a source sentence. Neural sequence-to-sequence models have gained considerable success for this task, while most existing approaches only focus on improving word overlap between the generated summary and the reference, which ignore the correctness, i.e., the summary should not contain error messages with respect to the source sentence. We argue that correctness is an essential requirement for summarization systems. Considering a correct summary is semantically entailed by the source sentence, we incorporate entailment knowledge into abstractive summarization models. We propose an entailment-aware encoder under multi-task framework (i.e., summarization generation and entailment recognition) and an entailment-aware decoder by entailment Reward Augmented Maximum Likelihood (RAML) training. Experimental results demonstrate that our models significantly outperform baselines from the aspects of informative-ness and correctness.},
author = {Li, Haoran and Zhu, Junnan and Zhang, Jiajun and Zong, Chengqing},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - Unknown - Ensure the Correctness of the Summary Incorporate Entailment Knowledge into Abstractive Sentence Summarization.pdf:pdf},
pages = {1430--1441},
title = {{Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization}},
url = {http://aclweb.org/anthology/C18-1121},
year = {2018}
}

@book{louviere2015best,
  title={Best-Worst Scaling: Theory, Methods and Applications},
  author={Louviere, J.J. and Flynn, T.N. and Marley, A.A.J.},
  isbn={9781107043152},
  lccn={2014044866},
  series={Cambridge books online},
  url={https://books.google.co.uk/books?id=W9uCCgAAQBAJ},
  year={2015},
  publisher={Cambridge University Press}
}

@InProceedings{vlachos-riedel:2014:W14-25,
  author    = {Vlachos, Andreas  and  Riedel, Sebastian},
  title     = {Fact Checking: Task definition and dataset construction},
  booktitle = {Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, MD, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {18--22},
  url       = {http://www.aclweb.org/anthology/W14-2508}
}


@inproceedings{Celikyilmaz2018,
abstract = {Phosphoribosyl pyrophosphate (PPRibP) synthetase activity was studied in cultured fibroblasts and lymphoblasts from a male child (patient 2-A) in whom inherited purine nucleotide and uric acid overproduction are accompanied by neurological deficits. Chromatographed or partially purified preparations of the child's enzyme showed 5-6-fold increased inhibitory constants (I0.5) for the noncompetitive inhibitors GDP and 6-methylthioinosine monophosphate but normal responsiveness to the competitive inhibitors ADP and 2,3-diphosphoglycerate. Activation of the PPRibP synthetase of patient 2-A by Piwas also abnormal with 3-4-fold reduced apparent KDvalues for Pi. Superactivity of the PPRibP synthetase of this child thus appeared to result from a combination of regulatory defects; selective resistance to noncompetitive inhibitors and increased responsiveness to Piactivation. Selective growth of the patient's fibroblasts in medium containing 6-methylthioinosine confirmed the functional significance of the in vitro inhibitor resistance of the aberrant enzyme. Fibroblasts and lymphoblasts derived from patient 2-A showed increased concentrations and rates of generation of PPRibP as well as increased rates of the pathways of purine base salvage and purine nucleotide synthesis de novo. The magnitudes of these increases in the child's cells exceeded those in cells with catalytically superactive PPRibP synthetases. These alterations as well as the in vitro kinetic abnormalities in the patient 2-A enzyme were expressed to a reduced degree in fibroblasts from the child's affected mother, supporting the proposal that this woman is a heterozygous carrier for X-linked enzyme superactivity. {\textcopyright} 1986.},
archivePrefix = {arXiv},
arxivId = {1803.10357},
author = {Celikyilmaz, Asli and Bosselut, Antoine and He, Xiaodong and Choi, Yejin},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
doi = {10.1016/0304-4165(86)90151-0},
eprint = {1803.10357},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Celikyilmaz et al. - Unknown - Deep Communicating Agents for Abstractive Summarization(2).pdf:pdf},
issn = {03044165},
title = {{Deep Communicating Agents for Abstractive Summarization}},
url = {https://arxiv.org/pdf/1803.10357.pdf http://arxiv.org/abs/1803.10357},
year = {2018}
}
@inproceedings{Tan2017,
address = {Stroudsburg, PA, USA},
author = {Tan, Jiwei and Wan, Xiaojun and Xiao, Jianguo},
booktitle = {Proceedings of the 55th Annual Meeting of the Association for
          Computational Linguistics (Volume 1: Long Papers)},
doi = {10.18653/v1/P17-1108},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan, Wan, Xiao - 2017 - Abstractive Document Summarization with a Graph-Based Attentional Neural Model.pdf:pdf},
pages = {1171--1181},
publisher = {Association for Computational Linguistics},
title = {{Abstractive Document Summarization with a Graph-Based Attentional Neural Model}},
url = {http://aclweb.org/anthology/P17-1108},
volume = {1},
year = {2017}
}

@book{Louviere2015,
author = {Louviere, Jordan J and Flynn, Terry N and Fred, Anthony Alfred and Marley, John},
publisher = {Cambridge University Press},
title = {{Best-worst scaling: Theory, methods and applications}},
year = {2015}
}
@article{Clarke2010,
abstract = {Sentence compression holds promise for many applications ranging from summarization to subtitle generation. The task is typically performed on isolated sentences without taking the surrounding context into account, even though most applications would operate over entire documents. In this article we present a discourse-informed model which is capable of producing document compressions that are coherent and informative. Our model is inspired by theories of local coherence and formulated within the framework of integer linear programming. Experimental results show significant improvements over a state-of-the-art discourse agnostic approach.},
author = {Clarke, James and Lapata, Mirella},
doi = {10.1162/coli_a_00004},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Clarke, Lapata - 2010 - Discourse Constraints for Document Compression.pdf:pdf},
isbn = {0891-2017},
issn = {08912017},
journal = {Computational Linguistics},
number = {3},
pages = {411--441},
pmid = {4120},
title = {{Discourse constraints for document compression}},
url = {https://www.mitpressjournals.org/doi/pdfplus/10.1162/coli{\_}a{\_}00004},
volume = {36},
year = {2010}
}
@article{Nenkova2004,
abstract = {We present an empirically grounded method for evaluating content selection in summariza- tion. It incorporates the idea that no single best model summary for a collection of documents exists. Our method quantifies the relative im- portance of facts to be conveyed. We argue that it is reliable, predictive and diagnostic, thus im- proves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.},
author = {Nenkova, A and Passonneau, R},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nenkova, Passonneau - Unknown - Evaluating Content Selection in Summarization The Pyramid Method.pdf:pdf},
journal = {Proceedings of HLT-NAACL},
keywords = {ani nenkova and rebecca,luating content selection in,passonneau,summarization,the pyramid method},
pages = {145--152},
title = {{Evaluating content selection in summarization: The pyramid method}},
url = {papers2://publication/uuid/DC675E84-0A45-48B7-A26C-F08B4B9398D3},
volume = {2004},
year = {2004}
}
@inproceedings{narayan18xsum,
author = {Narayan, Shashi and Cohen, Shay B. and Lapata, Mirella},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
title = {{Don't Give Me the Details, Just the Summary! Topic-aware Convolutional Neural Networks for Extreme Summarization.}},
year = {2018}
}
@inproceedings{Kryscinski2018,
abstract = {Abstractive text summarization aims to shorten long text documents into a human readable form that contains the most important facts from the original document. However, the level of actual abstraction as measured by novel phrases that do not appear in the source document remains low in existing approaches. We propose two techniques to improve the level of abstraction of generated summaries. First, we decompose the decoder into a contextual network that retrieves relevant parts of the source document, and a pretrained language model that incorporates prior knowledge about language generation. Second, we propose a novelty metric that is optimized directly through policy learning to encourage the generation of novel phrases. Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document.},
archivePrefix = {arXiv},
arxivId = {1808.07913},
author = {Kry{\'{s}}ci{\'{n}}ski, Wojciech and Paulus, Romain and Xiong, Caiming and Socher, Richard},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
eprint = {1808.07913},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kry{\'{s}}ci{\'{n}}ski et al. - 2018 - Improving Abstraction in Text Summarization.pdf:pdf},
pages = {1808--1817},
title = {{Improving Abstraction in Text Summarization}},
url = {http://arxiv.org/abs/1808.07913},
year = {2018}
}
@InProceedings{hardy2018sum,
  author = 	"Hardy, Hardy
		and Vlachos, Andreas",
  title = 	"Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation",
  booktitle = 	"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"768--773",
  location = 	"Brussels, Belgium",
  url = 	"http://aclweb.org/anthology/D18-1086"
}


@article{Bojar2016,
abstract = {This paper presents the results of the WMT16 shared tasks, which included five machine translation (MT) tasks (standard news, IT-domain, biomedical, multimodal, pronoun), three evaluation tasks (metrics, tuning, run-time estimation of MT qual-ity), and an automatic post-editing task and bilingual document alignment task. This year, 102 MT systems from 24 in-stitutions (plus 36 anonymized online sys-tems) were submitted to the 12 translation directions in the news translation task. The IT-domain task received 31 submissions from 12 institutions in 7 directions and the Biomedical task received 15 submissions systems from 5 institutions. Evaluation was both automatic and manual (relative ranking and 100-point scale assessments). The quality estimation task had three sub-tasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 en-tries.},
author = {Bojar, Ond{\v{r}}ej and Chatterjee, Rajen and Federmann, Christian and Graham, Yvette and Haddow, Barry and Huck, Matthias and {Jimeno Yepes}, Antonio and Koehn, Philipp and Logacheva, Varvara and Monz, Christof and Negri, Matteo and Neveol, Aurelie and Neves, Mariana and Popel, Martin and Post, Matt and Rubino, Raphael and Scarton, Carolina and Specia, Lucia and Turchi, Marco and Verspoor, Karin and Zampieri, Marcos},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bojar et al. - 2016 - Findings of the 2016 Conference on Machine Translation(2).pdf:pdf},
journal = {Proceedings of the First Conference on Machine Translation},
pages = {131--198},
title = {{Findings of the 2016 Conference on Machine Translation}},
url = {http://www.aclweb.org/anthology/W/W16/W16-2301},
volume = {2},
year = {2016}
}
@article{Likert1932,
author = {Likert, Rensis},
journal = {Archives of psychology},
title = {{A technique for the measurement of attitudes.}},
year = {1932}
}

@inproceedings{fomicheva2016reference,
  title={Reference bias in monolingual machine translation evaluation},
  author={Fomicheva, Marina and Specia, Lucia},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  volume={2},
  pages={77--82},
  year={2016}
}

@inproceedings{Gehrmann2018,
abstract = {Neural network-based methods for abstractive summarization produce outputs that are more fluent than other techniques, but which can be poor at content selection. This work proposes a simple technique for addressing this issue: use a data-efficient content selector to over-determine phrases in a source document that should be part of the summary. We use this selector as a bottom-up attention step to constrain the model to likely phrases. We show that this approach improves the ability to compress text, while still generating fluent summaries. This two-step process is both simpler and higher performing than other end-to-end content selection models, leading to significant improvements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the content selector can be trained with as little as 1,000 sentences, making it easy to transfer a trained summarizer to a new domain.},
archivePrefix = {arXiv},
arxivId = {1808.10792},
author = {Gehrmann, Sebastian and Deng, Yuntian and Rush, Alexander M},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
doi = {http://dx.doi.org/10.1016/j.jval.2015.09.2622},
eprint = {1808.10792},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gehrmann, Deng, Rush - Unknown - Bottom-Up Abstractive Summarization.pdf:pdf},
issn = {1098-3015},
pmid = {26533926},
title = {{Bottom-Up Abstractive Summarization}},
url = {https://arxiv.org/pdf/1808.10792.pdf http://arxiv.org/abs/1808.10792},
year = {2018}
}
@techreport{Krysci,
abstract = {ive text summarization aims to shorten long text documents into a human readable form that contains the most important facts from the original document. However, the level of actual abstraction as measured by novel phrases that do not appear in the source document remains low in existing approaches. We propose two techniques to improve the level of abstraction of generated summaries. First, we decompose the decoder into a contextual network that retrieves relevant parts of the source document, and a pretrained language model that incorporates prior knowledge about language generation. Second, we propose a novelty metric that is optimized directly through policy learning to encourage the generation of novel phrases. Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document.},
archivePrefix = {arXiv},
arxivId = {1808.07913v1},
author = {Krysc{\'{i}}, Wojciech and Nski, Krysc{\'{i}} and Paulus, Romain and Xiong, Caiming and Socher, Richard and Research, Salesforce},
eprint = {1808.07913v1},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krysc{\'{i}} et al. - Unknown - Improving Abstraction in Text Summarization.pdf:pdf},
title = {{Improving Abstraction in Text Summarization}},
url = {https://arxiv.org/pdf/1808.07913.pdf}
}

@inproceedings{Hermann2015,
abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
archivePrefix = {arXiv},
arxivId = {1506.03340},
author = {Hermann, Karl Moritz and Ko{\v{c}}isk{\'{y}}, Tom{\'{a}}{\v{s}} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
booktitle = {Neural Information Processing Systems},
doi = {10.1109/72.410363},
eprint = {1506.03340},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hermann et al. - 2015 - Teaching Machines to Read and Comprehend.pdf:pdf},
isbn = {1045-9227},
issn = {10495258},
pages = {1--14},
pmid = {18263409},
title = {{Teaching Machines to Read and Comprehend}},
url = {http://arxiv.org/abs/1506.03340},
year = {2015}
}
@article{ShafieiBavani2018,
author = {ShafieiBavani, Elaheh and Ebrahimi, Mohammad and Wong, Raymond and Chen, Fang},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/ShafieiBavani et al. - 2018 - Summarization Evaluation in the Absence of Human Model Summaries Using the Compositionality of Word Embedd.pdf:pdf},
journal = {Proceedings of the 27th International Conference on Computational Linguistics},
pages = {905--914},
title = {{Summarization Evaluation in the Absence of Human Model Summaries Using the Compositionality of Word Embeddings}},
url = {https://aclanthology.info/papers/C18-1077/c18-1077},
year = {2018}
}
@article{Hsu2018,
author = {Hsu, Wan-Ting and Lin, Chieh-Kai and Lee, Ming-Ying and Min, Kerui and Tang, Jing and Sun, Min},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsu et al. - 2018 - A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss.pdf:pdf},
journal = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
pages = {132--141},
title = {{A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss}},
url = {https://aclanthology.info/papers/P18-1013/p18-1013},
volume = {1},
year = {2018}
}
@article{Woodworth1991,
author = {Louviere, Jordan J. and Woodworth, George G.},
journal = {University of Alberta: Working Pa- per},
title = {{Best-worst scaling: A model for the largest difference judgments.}},
year = {1991}
}
