@inproceedings{dang2005overview,
author = {Dang, Hoa Trang},
booktitle = {Proceedings of the document understanding conference},
doi = {10.1.1.184.2425},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dang - Unknown - Overview of DUC 2005.pdf:pdf},
isbn = {1932432795},
mendeley-groups = {Experiments/Elly: Human Evaluation,Experiments/Elly: Human Evaluation/Common Practices,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {1--12},
title = {{Overview of DUC 2005}},
url = {http://www.isi.edu/ http://portal.acm.org/citation.cfm?doid=1654679.1654689},
volume = {2005},
year = {2005}
}
@article{Clarke2010,
abstract = {Sentence compression holds promise for many applications ranging from summarization to subtitle generation. The task is typically performed on isolated sentences without taking the surrounding context into account, even though most applications would operate over entire documents. In this article we present a discourse-informed model which is capable of producing document compressions that are coherent and informative. Our model is inspired by theories of local coherence and formulated within the framework of integer linear programming. Experimental results show significant improvements over a state-of-the-art discourse agnostic approach.},
author = {Clarke, James and Lapata, Mirella},
doi = {10.1162/coli_a_00004},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Clarke, Lapata - 2010 - Discourse Constraints for Document Compression.pdf:pdf},
isbn = {0891-2017},
issn = {08912017},
journal = {Computational Linguistics},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
number = {3},
pages = {411--441},
pmid = {4120},
title = {{Discourse constraints for document compression}},
url = {https://www.mitpressjournals.org/doi/pdfplus/10.1162/coli{\_}a{\_}00004},
volume = {36},
year = {2010}
}
@inproceedings{Pasunuru2018a,
abstract = {Abstractive text summarization is the task of compressing and rewriting a long document into a short summary while maintaining saliency, directed logical entailment, and non-redundancy. In this work, we address these three important aspects of a good summary via a reinforcement learning approach with two novel reward functions: ROUGESal and Entail, on top of a coverage-based baseline. The ROUGESal reward modifies the ROUGE metric by up-weighting the salient phrases/words detected via a keyphrase classifier. The Entail reward gives high (length-normalized) scores to logically-entailed summaries using an entailment classifier. Further, we show superior performance improvement when these rewards are combined with traditional metric (ROUGE) based rewards, via our novel and effective multi-reward approach of optimizing multiple rewards simultaneously in alternate mini-batches. Our method achieves the new state-of-the-art results (including human evaluation) on the CNN/Daily Mail dataset as well as strong improvements in a test-only transfer setup on DUC-2002.},
archivePrefix = {arXiv},
arxivId = {1804.06451},
author = {Pasunuru, Ramakanth and Bansal, Mohit},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
doi = {10.18653/v1/N18-2102},
eprint = {1804.06451},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pasunuru, Bansal - Unknown - Multi-Reward Reinforced Summarization with Saliency and Entailment.pdf:pdf},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {646--653},
title = {{Multi-Reward Reinforced Summarization with Saliency and Entailment}},
url = {http://aclweb.org/anthology/N18-2102 http://arxiv.org/abs/1804.06451},
year = {2018}
}
@article{Li2018a,
author = {Li, Haoran and Zhu, Junnan and Zhang, Jiajun and Zong, Chengqing},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - Unknown - Ensure the Correctness of the Summary Incorporate Entailment Knowledge into Abstractive Sentence Summarization.pdf:pdf},
journal = {Proceedings of the 27th International Conference on Computational Linguistics},
mendeley-groups = {Experiments/Elly: Human Evaluation/Summarization,Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {1430--1441},
title = {{Ensure the Correctness of the Summary: Incorporate Entailment Knowledge into Abstractive Sentence Summarization}},
url = {https://aclanthology.info/papers/C18-1121/c18-1121},
year = {2018}
}
@article{Nenkova2004a,
abstract = {We present an empirically grounded method for evaluating content selection in summariza- tion. It incorporates the idea that no single best model summary for a collection of documents exists. Our method quantifies the relative im- portance of facts to be conveyed. We argue that it is reliable, predictive and diagnostic, thus im- proves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.},
author = {Nenkova, Ani and Passonneau, Rebecca},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nenkova, Passonneau - Unknown - Evaluating Content Selection in Summarization The Pyramid Method.pdf:pdf},
journal = {Proceedings of HLT-NAACL},
mendeley-groups = {Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {145--152},
title = {{Evaluating content selection in summarization: The pyramid method}},
url = {http://www.isi.edu/ papers2://publication/uuid/DC675E84-0A45-48B7-A26C-F08B4B9398D3},
volume = {2004},
year = {2004}
}
@inproceedings{Peyrard2018a,
abstract = {Supervised summarization systems usually rely on supervision at the sentence or n-gram level provided by automatic metrics like ROUGE, which act as noisy proxies for human judgments. In this work, we learn a summary-level scoring function $\theta$ including human judgments as supervision and automatically generated data as regularization. We extract summaries with a genetic algorithm using $\theta$ as a fitness function. We observe strong and promising performances across datasets in both automatic and manual evaluation.},
author = {Peyrard, Maxime and Gurevych, Iryna},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
doi = {10.18653/v1/N18-2103},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peyrard, Gurevych - Unknown - Objective Function Learning to Match Human Judgements for Optimization-Based Summarization.pdf:pdf},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
pages = {654--660},
title = {{Objective Function Learning to Match Human Judgements for Optimization-Based Summarization}},
url = {http://tac.nist.gov/2008/},
year = {2018}
}
@inproceedings{Hardy2018,
abstract = {Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using gold standard AMR parses and parses obtained from an off-the-shelf parser respectively. We also find that the summarization performance using the latter is 2 ROUGE-2 points higher than that of a well-established neural encoder-decoder approach trained on a larger dataset. Code is available at $\backslash$url{\{}https://github.com/sheffieldnlp/AMR2Text-summ{\}}},
archivePrefix = {arXiv},
arxivId = {1808.09160},
author = {Hardy and Vlachos, Andreas},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.},
eprint = {1808.09160},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hardy, Vlachos - 2018 - Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation.pdf:pdf},
mendeley-groups = {Report and Paper/18 Months,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
month = {aug},
pages = {768--773},
title = {{Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation}},
url = {http://arxiv.org/abs/1808.09160},
year = {2018}
}
@article{Nallapati2016a,
abstract = {In this work, we cast abstractive text summarization as a sequence-to-sequence problem and employ the framework of Attentional Encoder-Decoder Recurrent Neural Networks to this problem, outperforming state-of-the art model of Rush et. al. (2015) on two different corpora. We also move beyond the basic architecture, and propose several novel models to address important problems in summarization including modeling key-words, capturing the hierarchy of sentence-to-word structure and addressing the problem of words that are key to a document, but rare elsewhere. Our work shows that many of our proposed solutions contribute to further improvement in performance. In addition, we propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.},
archivePrefix = {arXiv},
arxivId = {1602.06023},
author = {Nallapati, Ramesh and Zhou, Bowen and dos Santos, Cicero Nogueira and Gulcehre, Caglar and Xiang, Bing},
eprint = {1602.06023},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nallapati et al. - 2016 - Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond.pdf:pdf},
journal = {Proceedings of CoNLL},
mendeley-groups = {Academic/NLP/Summarization,Report and Paper/18 Months,Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
pages = {280--290},
title = {{Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond}},
url = {http://arxiv.org/abs/1602.06023},
year = {2016}
}
@article{Josep1971,
author = {Fleiss, Joseph L.},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - full-text(3).pdf:pdf},
journal = {Psychological Bulletin},
mendeley-groups = {Academic/Math,Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
number = {5},
pages = {378--382},
title = {{Measuring Nominal Scale Agreement Among Many Raters}},
volume = {76},
year = {1971}
}
@inproceedings{Liao2018a,
abstract = {Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to multi-document summarization have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Rep-resentation (AMR), a semantic representation of natural language grounded in linguistic theory, as a form of content representation. Our approach condenses source documents to a set of sum-mary graphs following the AMR formalism. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The framework is fully data-driven and flexi-ble. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research.},
author = {Liao, Kexin and Lebanoff, Logan and Liu, Fei},
booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liao, Lebanoff, Liu - Unknown - Abstract Meaning Representation for Multi-Document Summarization.pdf:pdf},
mendeley-groups = {Academic/NLP/AMR,Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {1178--1190},
title = {{Abstract Meaning Representation for Multi-Document Summarization}},
url = {https://arxiv.org/pdf/1806.05655.pdf},
year = {2018}
}
@article{Cao2018a,
author = {Cao, Ziqiang and Li, Wenjie and Li, Sujian and Wei, Furu},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao et al. - 2018 - Retrieve, Rerank and Rewrite Soft Template Based Neural Summarization(3).pdf:pdf},
journal = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
pages = {152--161},
title = {{Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization}},
url = {https://aclanthology.info/papers/P18-1015/p18-1015},
volume = {1},
year = {2018}
}
@inproceedings{Guo2018a,
abstract = {An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document. We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning-worthy details, and the latter teaches the model how to rewrite a summary which is a directed-logical subset of the input document. We also propose novel multi-task architectures with high-level (semantic) layer-specific sharing across multiple encoder and decoder layers of the three tasks, as well as soft-sharing mechanisms (and show performance ablations and analysis examples of each contribution). Overall, we achieve statistically significant improvements over the state-of-the-art on both the CNN/DailyMail and Gigaword datasets, as well as on the DUC-2002 transfer setup. We also present several quantitative and qualitative analysis studies of our model's learned saliency and entailment skills.},
archivePrefix = {arXiv},
arxivId = {1805.11004},
author = {Guo, Han and Pasunuru, Ramakanth and Bansal, Mohit},
booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
eprint = {1805.11004},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo, Pasunuru - Unknown - Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation.pdf:pdf},
issn = {1528-3356},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {687--697},
title = {{Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation}},
url = {http://aclweb.org/anthology/P18-1064 http://arxiv.org/abs/1805.11004},
year = {2018}
}
@article{Bojar2016,
abstract = {This paper presents the results of the WMT16 shared tasks, which included five machine translation (MT) tasks (standard news, IT-domain, biomedical, multimodal, pronoun), three evaluation tasks (metrics, tuning, run-time estimation of MT qual-ity), and an automatic post-editing task and bilingual document alignment task. This year, 102 MT systems from 24 in-stitutions (plus 36 anonymized online sys-tems) were submitted to the 12 translation directions in the news translation task. The IT-domain task received 31 submissions from 12 institutions in 7 directions and the Biomedical task received 15 submissions systems from 5 institutions. Evaluation was both automatic and manual (relative ranking and 100-point scale assessments). The quality estimation task had three sub-tasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 en-tries.},
author = {Bojar, Ond{\v{r}}ej and Chatterjee, Rajen and Federmann, Christian and Graham, Yvette and Haddow, Barry and Huck, Matthias and {Jimeno Yepes}, Antonio and Koehn, Philipp and Logacheva, Varvara and Monz, Christof and Negri, Matteo and Neveol, Aurelie and Neves, Mariana and Popel, Martin and Post, Matt and Rubino, Raphael and Scarton, Carolina and Specia, Lucia and Turchi, Marco and Verspoor, Karin and Zampieri, Marcos},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bojar et al. - 2016 - Findings of the 2016 Conference on Machine Translation(2).pdf:pdf},
journal = {Proceedings of the First Conference on Machine Translation},
mendeley-groups = {Academic/NLP/Machine Translation,Experiments/Belle: AMR-to-Text for Summ,Experiments/Belle: AMR-to-Text for Summ/Dataset-Evaluation,Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
pages = {131--198},
title = {{Findings of the 2016 Conference on Machine Translation}},
url = {http://www.aclweb.org/anthology/W/W16/W16-2301},
volume = {2},
year = {2016}
}
@unpublished{Woodworth1991,
author = {Woodworth, Jordan J Louviere and George G},
booktitle = {Working Paper},
institution = {University of Alberta},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
title = {{Best-worst scaling: A model for the largest difference judgments.}},
year = {1991}
}
@misc{Sandhaus2008,
address = {Philadelphia},
author = {Sandhaus, Evan},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
publisher = {Linguistic Data Consortium},
title = {{The New York Times Annotated Corpus}},
year = {2008}
}
@inproceedings{See2017,
abstract = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
archivePrefix = {arXiv},
arxivId = {1704.04368},
author = {See, Abigail and Liu, Peter J. and Manning, Christopher D.},
booktitle = {Proceedings of the 55th Annual Meeting of the ACL},
doi = {10.18653/v1/P17-1099},
eprint = {1704.04368},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/See, Liu, Manning - 2017 - Get To The Point Summarization with Pointer-Generator Networks(2).pdf:pdf},
isbn = {9781945626753},
mendeley-groups = {Academic/NLP/Summarization,Report and Paper/EMNLP 2018,Experiments/Chloe: AMR Summarization Full,Report and Paper/18 Months,Experiments/Elly: Human Evaluation/Summarization,Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
title = {{Get To The Point: Summarization with Pointer-Generator Networks}},
url = {http://arxiv.org/abs/1704.04368},
year = {2017}
}
@inproceedings{Celikyilmaz2018,
abstract = {Phosphoribosyl pyrophosphate (PPRibP) synthetase activity was studied in cultured fibroblasts and lymphoblasts from a male child (patient 2-A) in whom inherited purine nucleotide and uric acid overproduction are accompanied by neurological deficits. Chromatographed or partially purified preparations of the child's enzyme showed 5-6-fold increased inhibitory constants (I0.5) for the noncompetitive inhibitors GDP and 6-methylthioinosine monophosphate but normal responsiveness to the competitive inhibitors ADP and 2,3-diphosphoglycerate. Activation of the PPRibP synthetase of patient 2-A by Piwas also abnormal with 3-4-fold reduced apparent KDvalues for Pi. Superactivity of the PPRibP synthetase of this child thus appeared to result from a combination of regulatory defects; selective resistance to noncompetitive inhibitors and increased responsiveness to Piactivation. Selective growth of the patient's fibroblasts in medium containing 6-methylthioinosine confirmed the functional significance of the in vitro inhibitor resistance of the aberrant enzyme. Fibroblasts and lymphoblasts derived from patient 2-A showed increased concentrations and rates of generation of PPRibP as well as increased rates of the pathways of purine base salvage and purine nucleotide synthesis de novo. The magnitudes of these increases in the child's cells exceeded those in cells with catalytically superactive PPRibP synthetases. These alterations as well as the in vitro kinetic abnormalities in the patient 2-A enzyme were expressed to a reduced degree in fibroblasts from the child's affected mother, supporting the proposal that this woman is a heterozygous carrier for X-linked enzyme superactivity. {\textcopyright} 1986.},
archivePrefix = {arXiv},
arxivId = {1803.10357},
author = {Celikyilmaz, Asli and Bosselut, Antoine and He, Xiaodong and Choi, Yejin},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
doi = {10.1016/0304-4165(86)90151-0},
eprint = {1803.10357},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Celikyilmaz et al. - Unknown - Deep Communicating Agents for Abstractive Summarization(2).pdf:pdf},
issn = {03044165},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
title = {{Deep Communicating Agents for Abstractive Summarization}},
url = {https://arxiv.org/pdf/1803.10357.pdf http://arxiv.org/abs/1803.10357},
year = {2018}
}
@article{Song2018,
author = {Song, Kaiqiang and Zhao, Lin and Liu, Fei},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Song, Zhao, Liu - Unknown - Structure-Infused Copy Mechanisms for Abstractive Summarization.pdf:pdf},
journal = {Proceedings of the 27th International Conference on Computational Linguistics},
mendeley-groups = {Experiments/Elly: Human Evaluation/Summarization,Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {1717--1729},
title = {{Structure-Infused Copy Mechanisms for Abstractive Summarization}},
url = {https://aclanthology.info/papers/C18-1146/c18-1146},
year = {2018}
}
@inproceedings{Kryscinski2018,
abstract = {Abstractive text summarization aims to shorten long text documents into a human readable form that contains the most important facts from the original document. However, the level of actual abstraction as measured by novel phrases that do not appear in the source document remains low in existing approaches. We propose two techniques to improve the level of abstraction of generated summaries. First, we decompose the decoder into a contextual network that retrieves relevant parts of the source document, and a pretrained language model that incorporates prior knowledge about language generation. Second, we propose a novelty metric that is optimized directly through policy learning to encourage the generation of novel phrases. Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document.},
archivePrefix = {arXiv},
arxivId = {1808.07913},
author = {Kry{\'{s}}ci{\'{n}}ski, Wojciech and Paulus, Romain and Xiong, Caiming and Socher, Richard},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
eprint = {1808.07913},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kry{\'{s}}ci{\'{n}}ski et al. - 2018 - Improving Abstraction in Text Summarization.pdf:pdf},
mendeley-groups = {Experiments/Elly: Human Evaluation/Summarization,Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
pages = {1808--1817},
title = {{Improving Abstraction in Text Summarization}},
url = {http://arxiv.org/abs/1808.07913},
year = {2018}
}
@article{Bojar2011,
author = {Bojar, Ond{\v{r}}ej and Ercegov{\v{c}}evi{\'{c}}, Milo{\v{s}} and Popel, Martin and Zaidan, Omar},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bojar et al. - 2011 - A Grain of Salt for the WMT Manual Evaluation(2).pdf:pdf},
isbn = {978-1-937284-12-1},
journal = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
mendeley-groups = {Academic/NLP/Machine Translation,Experiments/Belle: AMR-to-Text for Summ/Dataset-Evaluation,Experiments/Elly: Human Evaluation,Experiments/Elly: Human Evaluation/Related approach,Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
pages = {1--11},
title = {{A Grain of Salt for the WMT Manual Evaluation}},
url = {http://www.aclweb.org/anthology/W11-2101},
year = {2011}
}
@inproceedings{Hermann2015,
abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
archivePrefix = {arXiv},
arxivId = {1506.03340},
author = {Hermann, Karl Moritz and Ko{\v{c}}isk{\'{y}}, Tom{\'{a}}{\v{s}} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
booktitle = {Neural Information Processing Systems},
doi = {10.1109/72.410363},
eprint = {1506.03340},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hermann et al. - 2015 - Teaching Machines to Read and Comprehend.pdf:pdf},
isbn = {1045-9227},
issn = {10495258},
mendeley-groups = {Report and Paper/EMNLP 2018,Report and Paper/18 Months,Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
pages = {1--14},
pmid = {18263409},
title = {{Teaching Machines to Read and Comprehend}},
url = {http://arxiv.org/abs/1506.03340},
year = {2015}
}
@inproceedings{Cohan2018a,
abstract = {Neural abstractive summarization models have led to promising results in summarizing relatively short documents. We propose the first model for abstractive summarization of single, longer-form documents (e.g., research papers). Our approach consists of a new hierarchical encoder that models the discourse structure of a document, and an attentive discourse-aware decoder to generate the summary. Empirical results on two large-scale datasets of scientific papers show that our model significantly out-performs state-of-the-art models.},
author = {Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers).},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohan, Dernoncourt Doo Soon Kim Trung Bui Seokhwan Kim Walter Chang Nazli Goharian - Unknown - A Discourse-Aware Attention Model for Abs.pdf:pdf},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
pages = {615--621},
title = {{A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents}},
url = {https://github.com/acohan/long-summarization},
year = {2018}
}
@article{Robertson1946,
author = {Robertson, D. W.},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Robertson - 1946 - A Note on the Classical Origin of Circumstances in the Medieval Confessional.pdf:pdf},
journal = {Studies in Philology},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
number = {1},
pages = {6--14},
title = {{A Note on the Classical Origin of " Circumstances " in the Medieval Confessional}},
volume = {43},
year = {1946}
}
@article{Hsu2018,
author = {Hsu, Wan-Ting and Lin, Chieh-Kai and Lee, Ming-Ying and Min, Kerui and Tang, Jing and Sun, Min},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsu et al. - 2018 - A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss.pdf:pdf},
journal = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
mendeley-groups = {Experiments/Elly: Human Evaluation/Summarization,Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
pages = {132--141},
title = {{A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss}},
url = {https://aclanthology.info/papers/P18-1013/p18-1013},
volume = {1},
year = {2018}
}
@inproceedings{Krishna2018a,
abstract = {Summarizing a document requires identifying the important parts of the document with an objective of providing a quick overview to a reader. However, a long article can span sev-eral topics and a single summary cannot do justice to all the topics. Further, the interests of readers can vary and the notion of impor-tance can change across them. Existing sum-marization algorithms generate a single sum-mary and are not capable of generating mul-tiple summaries tuned to the interests of the readers. In this paper, we propose an attention based RNN framework to generate multiple summaries of a single document tuned to dif-ferent topics of interest. Our method outper-forms existing baselines and our results sug-gest that the attention of generative networks can be successfully biased to look at sentences relevant to a topic and effectively used to gen-erate topic-tuned summaries.},
author = {Krishna, Kundan and Srinivasan, Balaji Vasan},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
doi = {10.18653/v1/n18-1153},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krishna, Research, Srinivasan - Unknown - Generating topic-oriented summaries using neural attention.pdf:pdf},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
pages = {1697--1705},
title = {{Generating Topic-Oriented Summaries Using Neural Attention}},
url = {http://aclweb.org/anthology/N18-1153},
year = {2018}
}
@inproceedings{Yang2017b,
abstract = {We present a robust approach for detecting intrinsic sentence importance in news, by training on two corpora of document-summary pairs. When used for single-document summarization, our approach, combined with the "beginning of document" heuristic, outperforms a state-of-the-art summarizer and the beginning-of-article baseline in both automatic and manual evaluations. These results represent an important advance because in the absence of cross-document repetition, single document summarizers for news have not been able to consistently outperform the strong beginning-of-article baseline.},
archivePrefix = {arXiv},
arxivId = {1702.07998},
author = {Yang, Yinfei and Bao, Forrest Sheng and Nenkova, Ani},
booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
eprint = {1702.07998},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Sheng Bao, Nenkova - Unknown - Detecting (Un)Important Content for Single-Document News Summarization.pdf:pdf},
mendeley-groups = {Experiments/Elly: Human Evaluation/Summarization,Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {707--712},
title = {{Detecting (Un)Important Content for Single-Document News Summarization}},
url = {https://aclanthology.info/papers/E17-2112/e17-2112 http://arxiv.org/abs/1702.07998},
volume = {2},
year = {2017}
}
@inproceedings{ShafieiBavani2018,
author = {ShafieiBavani, Elaheh and Ebrahimi, Mohammad and Wong, Raymond and Chen, Fang},
booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/ShafieiBavani et al. - 2018 - Summarization Evaluation in the Absence of Human Model Summaries Using the Compositionality of Word Embedd.pdf:pdf},
mendeley-groups = {Experiments/Elly: Human Evaluation/Summarization,Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
pages = {905--914},
title = {{Summarization Evaluation in the Absence of Human Model Summaries Using the Compositionality of Word Embeddings}},
url = {https://aclanthology.info/papers/C18-1077/c18-1077},
year = {2018}
}
@inproceedings{schluter:2017:EACLshort,
author = {Schluter, Natalie},
booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schluter - 2017 - The limits of automatic summarisation according to ROUGE.pdf:pdf},
mendeley-groups = {Experiments/Elly: Human Evaluation/Automatic Evaluation,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {41--45},
title = {{The limits of automatic summarisation according to ROUGE}},
url = {https://aclanthology.info/papers/E17-2007/e17-2007},
volume = {2},
year = {2017}
}
@inproceedings{Amplayo2018a,
abstract = {A major proportion of a text summary includes important entities found in the original text. These entities build up the topic of the summary. Moreover, they hold commonsense information once they are linked to a knowledge base. Based on these observations, this paper investigates the usage of linked entities to guide the decoder of a neural text summarizer to generate concise and better summaries. To this end, we leverage on an off-the-shelf entity linking system (ELS) to extract linked entities and propose Entity2Topic (E2T), a module easily attachable to a sequence-to-sequence model that transforms a list of entities into a vector representation of the topic of the summary. Current available ELS's are still not sufficiently effective, possibly introducing unresolved ambiguities and irrelevant entities. We resolve the imperfections of the ELS by (a) encoding entities with selective disambiguation, and (b) pooling entity vectors using firm attention. By applying E2T to a simple sequence-to-sequence model with attention mechanism as base model, we see significant improvements of the performance in the Gigaword (sentence to title) and CNN (long document to multi-sentence highlights) summarization datasets by at least 2 ROUGE points.},
archivePrefix = {arXiv},
arxivId = {1806.05504},
author = {Amplayo, Reinald Kim and Lim, Seonjae and Hwang, Seung-Won},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
doi = {10.18653/v1/N18-1064},
eprint = {1806.05504},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Amplayo, Lim, Hwang - Unknown - Entity Commonsense Representation for Neural Abstractive Summarization.pdf:pdf},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
pages = {697--707},
title = {{Entity Commonsense Representation for Neural Abstractive Summarization}},
url = {https://github.com/idio/wiki2vec http://arxiv.org/abs/1806.05504},
year = {2018}
}
@article{Louis2013,
archivePrefix = {arXiv},
arxivId = {1604.07370},
author = {Louis, Annie and Nenkova, Ani},
doi = {10.1162/COLI},
eprint = {1604.07370},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Louis, Nenkova - 2013 - Automatically Assessing Machine Summary Content Without a Gold Standard.pdf:pdf},
isbn = {9781608459858},
issn = {01272713},
journal = {Computational Linguistics},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
number = {2},
pages = {267--300},
pmid = {22251136},
title = {{Automatically Assessing Machine Summary Content Without a Gold Standard}},
url = {http://www.seas.upenn.edu/},
volume = {39},
year = {2013}
}
@inproceedings{Kedzie2018,
abstract = {We carry out experiments with deep learning models of summarization across the domains of news, personal stories, meetings, and medical articles in order to understand how content selection is performed. We find that many sophisticated features of state of the art extractive summarizers do not improve performance over simpler models. These results suggest that it is easier to create a summarizer for a new domain than previous work suggests and bring into question the benefit of deep learning models for summarization for those domains that do have massive datasets (i.e., news). At the same time, they suggest important questions for new research in summarization; namely, new forms of sentence representations or external knowledge sources are needed that are better suited to the sumarization task.},
address = {Brussels, Belgium},
archivePrefix = {arXiv},
arxivId = {1810.12343v1},
author = {Kedzie, Chris and McKeown, Kathleen and {Daum{\'{e}} III}, Hal},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
eprint = {1810.12343v1},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kedzie, McKeown, Daum{\'{e}} III - Unknown - Content Selection in Deep Learning Models of Summarization.pdf:pdf},
mendeley-groups = {Experiments/Elly: Human Evaluation/Summarization,Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
pages = {1818----1828},
publisher = {Association for Computational Linguistics},
title = {{Content Selection in Deep Learning Models of Summarization}},
url = {https://github.com/kedz/nnsum/},
year = {2018}
}
@inproceedings{Paulus2018,
abstract = {Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. However, for longer documents and summaries, these models often include repetitive and incoherent phrases. We introduce a neural network model with intra-attention and a new training method. This method combines standard supervised word prediction and reinforcement learning (RL). Models trained only with the former often exhibit "exposure bias" -- they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, a 5.7 absolute points improvement over previous state-of-the-art models. It also performs well as the first abstractive model on the New York Times corpus. Human evaluation also shows that our model produces higher quality summaries.},
archivePrefix = {arXiv},
arxivId = {1705.04304},
author = {Paulus, Romain and Xiong, Caiming and Socher, Richard},
booktitle = {Proceedings of the 6th International Conference on Learning Representations},
eprint = {1705.04304},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paulus, Xiong, Socher - 2018 - A Deep Reinforced Model for Abstractive Summarization(2).pdf:pdf},
mendeley-groups = {Academic/NLP/Summarization,Reading Group,Experiments/Belle: AMR-to-Text for Summ/Method-Copying,Report and Paper/EMNLP 2018,Experiments/Chloe: AMR Summarization Full,Report and Paper/18 Months,Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
title = {{A Deep Reinforced Model for Abstractive Summarization}},
url = {http://arxiv.org/abs/1705.04304},
year = {2018}
}
@inproceedings{Sakaue2018a,
abstract = {Submodular maximization with the greedy algorithm has been studied as an effective approach to extractive summarization. This approach is known to have three advan-tages: its applicability to many useful sub-modular objective functions, the efficiency of the greedy algorithm, and the provable performance guarantee. However, when it comes to compressive summarization, we are currently missing a counterpart of the extractive method based on submodular-ity. In this paper, we propose a fast greedy method for compressive summarization. Our method is applicable to any mono-tone submodular objective function, includ-ing many functions well-suited for docu-ment summarization. We provide an ap-proximation guarantee of our greedy algo-rithm. Experiments show that our method is about 100 to 400 times faster than an existing method based on integer-linear-programming (ILP) formulations and that our method empirically achieves more than 95{\%}-approximation.},
author = {Sakaue, Shinsaku and Hirao, Tsutomu and Nishino, Masaaki and Nagata, Masaaki},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
doi = {10.18653/v1/n18-1157},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sakaue et al. - Unknown - Provable Fast Greedy Compressive Summarization with Any Monotone Submodular Function.pdf:pdf},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
pages = {1737--1746},
title = {{Provable Fast Greedy Compressive Summarization with Any Monotone Submodular Function}},
url = {http://aclweb.org/anthology/N18-1157},
year = {2018}
}
@article{Bojar2017,
author = {Bojar, Ond{\v{r}}ej and Chatterjee, Rajen and Federmann, Christian and Graham, Yvette and Haddow, Barry and Huang, Shujian and Huck, Matthias and Koehn, Philipp and Liu, Qun and Logacheva, Varvara and Monz, Christof and Negri, Matteo and Post, Matt and Rubino, Raphael and Specia, Lucia and Turchi, Marco},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bojar et al. - 2017 - Findings of the 2017 Conference on Machine Translation (WMT17)(2).pdf:pdf},
journal = {Wmt-2017},
mendeley-groups = {Academic/NLP/Machine Translation,Experiments/Belle: AMR-to-Text for Summ/Dataset-Evaluation,Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
title = {{Findings of the 2017 Conference on Machine Translation (WMT17)}},
year = {2017}
}
@article{Jadhav2018,
author = {Jadhav, Aishwarya and Rajan, Vaibhav},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jadhav, Rajan - Unknown - Extractive Summarization with SWAP-NET Sentences and Words from Alternating Pointer Networks.pdf:pdf},
journal = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
mendeley-groups = {Experiments/Elly: Human Evaluation/Summarization,Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {142--151},
title = {{Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks}},
url = {https://aclanthology.info/papers/P18-1014/p18-1014},
volume = {1},
year = {2018}
}
@inproceedings{narayan18xsum,
author = {Narayan, Shashi and Cohen, Shay B. and Lapata, Mirella},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - full-text(2).pdf:pdf},
mendeley-groups = {Academic/NLP/Summarization,Experiments/Elly: Human Evaluation/Summarization,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
title = {{Don't Give Me the Details, Just the Summary! Topic-aware Convolutional Neural Networks for Extreme Summarization.}},
year = {2018}
}
@article{Federmann2012,
abstract = {We describe a focused effort to investigate the performance of phrase-based,human evaluation of machine translation output achieving a high annotatoragreement. We define phrase-based evaluation and describe the implementation ofAppraise, a toolkit that supports the manual evaluation of machine translationresults. Phrase ranking can be done using either a fine-grained six-way scoringscheme that allows to differentiate between "much better" and "slightlybetter", or a reduced subset of ranking choices. Afterwards we discuss kappavalues for both scoring models from several experiments conducted with humanannotators. Our results show that phrase-based evaluation can be used for fastevaluation obtaining significant agreement among annotators. The granularity ofranking choices should, however, not be too fine-grained as this seems toconfuse annotators and thus reduces the overall agreement. The work reported inthis paper confirms previous work in the field and illustrates that the usageof human evaluation in machine translation should be reconsidered. The Appraisetoolkit is available as open-source and can be downloaded from the author'swebsite.},
author = {Federmann, Christian},
doi = {10.2478/v10108-012-0006-9},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Federmann - 2012 - Appraise an Open-Source Toolkit for Manual Evaluation of MT Output(2).pdf:pdf},
isbn = {2-9517408-6-7},
issn = {1804-0462},
journal = {The Prague Bulletin of Mathematical Linguistics},
keywords = {applications,evaluation,machine translation},
mendeley-groups = {Academic/NLP/Machine Translation,Experiments/Belle: AMR-to-Text for Summ,Experiments/Belle: AMR-to-Text for Summ/Dataset-Evaluation,Experiments/Elly: Human Evaluation/Related approach,Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
number = {-1},
pages = {130--134},
title = {{Appraise: an Open-Source Toolkit for Manual Evaluation of MT Output}},
url = {http://www.degruyter.com/view/j/pralin.2012.98.issue--1/v10108-012-0006-9/v10108-012-0006-9.xml},
volume = {98},
year = {2012}
}
@inproceedings{Tan2017,
address = {Stroudsburg, PA, USA},
author = {Tan, Jiwei and Wan, Xiaojun and Xiao, Jianguo},
booktitle = {Proceedings of the 55th Annual Meeting of the Association for
          Computational Linguistics (Volume 1: Long Papers)},
doi = {10.18653/v1/P17-1108},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan, Wan, Xiao - 2017 - Abstractive Document Summarization with a Graph-Based Attentional Neural Model.pdf:pdf},
mendeley-groups = {Experiments/Elly: Human Evaluation/Summarization,Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
pages = {1171--1181},
publisher = {Association for Computational Linguistics},
title = {{Abstractive Document Summarization with a Graph-Based Attentional Neural Model}},
url = {http://aclweb.org/anthology/P17-1108},
volume = {1},
year = {2017}
}
@book{Louviere2015,
author = {Louviere, Jordan J and Flynn, Terry N and Fred, Anthony Alfred and Marley, John},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
publisher = {Cambridge University Press},
title = {{Best-worst scaling: Theory, methods and applications}},
year = {2015}
}
@article{Chen2018a,
author = {Chen, Yen-Chun and Bansal, Mohit},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Bansal - 2018 - Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting(2).pdf:pdf},
journal = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
mendeley-groups = {Experiments/Elly: Human Evaluation/Summarization,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {675--686},
title = {{Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting}},
url = {https://aclanthology.info/papers/P18-1063/p18-1063},
volume = {1},
year = {2018}
}
@article{Likert1932,
author = {Likert, Rensis},
journal = {Archives of psychology},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
title = {{A technique for the measurement of attitudes.}},
year = {1932}
}
@inproceedings{Lin2004,
abstract = {ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to auto- matically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of over- lapping units such as n-gram, word sequences, and word pairs between the computer-generated sum- mary to be evaluated and the ideal summaries cre- ated by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summariza- tion evaluation package and their evaluatio ns. Three of them have been used in the Document Under- standing$\backslash$tConference$\backslash$t(DUC)$\backslash$t2004,$\backslash$ta$\backslash$tlarge -scale summarization evaluation sponsored by NIST.},
author = {Lin, Chin-Yew},
booktitle = {Proceedings of the workshop on Text Summarization Branches Out, Post-Conference Workshop of ACL 2004},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin - 2004 - Rouge A package for automatic evaluation of summaries.pdf:pdf},
issn = {00036951},
keywords = {2004,a package for automatic,barcelona,evaluation of summaries,post-conference workshop of acl,proceedings of workshop on,r ouge,spain,text summarization branches out},
mendeley-groups = {Academic/NLP/Summarization,Experiments/Belle: AMR-to-Text for Summ/Dataset-Evaluation,Academic,Report and Paper/18 Months,Experiments/Elly: Human Evaluation,Experiments/Elly: Human Evaluation/Automatic Evaluation,Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
number = {1},
pages = {25--26},
title = {{Rouge: A package for automatic evaluation of summaries}},
url = {papers2://publication/uuid/5DDA0BB8-E59F-44C1-88E6-2AD316DAEF85},
year = {2004}
}

@inproceedings{Fan2018,
abstract = {Current models for document summarization disregard user preferences such as the desired length, style, the entities that the user might be interested in, or how much of the document the user has already read. We present a neural summarization model with a simple but effective mechanism to enable users to specify these high level attributes in order to control the shape of the final summaries to better suit their needs. With user input, our system can produce high quality summaries that follow user preferences. Without user input, we set the control variables automatically. On the full text CNN-Dailymail dataset, we outperform state of the art abstractive systems (both in terms of F1-ROUGE1 40.38 vs. 39.53 and human evaluation).},
archivePrefix = {arXiv},
arxivId = {1711.05217},
author = {Fan, Angela and Grangier, David and Auli, Michael},
booktitle = {ACL 2018 Workshop on Neural Machine Translation and Generation},
eprint = {1711.05217},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fan, Grangier, Auli - Unknown - Controllable Abstractive Summarization.pdf:pdf},
mendeley-groups = {Report and Paper/ACL 2019 (old),Report and Paper,Report and Paper/ACL 2019},
title = {{Controllable Abstractive Summarization}},
url = {https://arxiv.org/pdf/1711.05217.pdf http://arxiv.org/abs/1711.05217},
year = {2018}
}
@book{everitt2006cambridge,
author = {Everitt, Brian S},
mendeley-groups = {Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
publisher = {Cambridge University Press},
title = {{The Cambridge dictionary of statistics}},
year = {2006}
}
@inproceedings{Nenkova:2005:ATS,
author = {Nenkova, Ani},
booktitle = {Proceedings of the 20th National Conference on Artificial Intelligence - Volume 3},
mendeley-groups = {Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {1436--1441},
title = {{Automatic Text Summarization of Newswire: Lessons Learned from the Document Understanding Conference}},
year = {2005}
}
@inproceedings{narayan-sidenet18,
address = {Melbourne, Australia},
author = {Narayan, Shashi and Cardenas, Ronald and Papasarantopoulos, Nikos and Cohen, Shay B and Lapata, Mirella and Yu, Jiangsheng and Chang, Yi},
booktitle = {Proceedings of the 56st Annual Meeting of the Association for Computational Linguistics},
mendeley-groups = {Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {2020--2030},
title = {{Document Modeling with External Attention for Sentence Extraction}},
year = {2018}
}
@inproceedings{Narayan2018,
address = {Stroudsburg, PA, USA},
author = {Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
mendeley-groups = {Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {1747--1759},
publisher = {Association for Computational Linguistics},
title = {{Ranking Sentences for Extractive Summarization with Reinforcement Learning}},
url = {http://aclweb.org/anthology/N18-1158},
volume = {1},
year = {2018}
}
@article{Lin2018,
abstract = {In neural abstractive summarization, the conventional sequence-to-sequence (seq2seq) model often suffers from repetition and semantic irrelevance. To tackle the problem, we propose a global encoding framework, which controls the information flow from the encoder to the decoder based on the global information of the source context. It consists of a convolutional gated unit to perform global encoding to improve the representations of the source-side information. Evaluations on the LCSTS and the English Gigaword both demonstrate that our model outperforms the baseline models, and the analysis shows that our model is capable of reducing repetition.},
archivePrefix = {arXiv},
arxivId = {1805.03989},
author = {Lin, Junyang and Ma, Shuming and Su, Qi},
doi = {10.1111/jcpp.12768},
eprint = {1805.03989},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - Unknown - Global Encoding for Abstractive Summarization.pdf:pdf},
journal = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
mendeley-groups = {Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {163--169},
title = {{Global Encoding for Abstractive Summarization}},
url = {https://www.github.},
year = {2018}
}
@inproceedings{newsroom_N181065,
author = {Grusky, Max and Naaman, Mor and Artzi, Yoav},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
mendeley-groups = {Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {708--719},
publisher = {Association for Computational Linguistics},
title = {{Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies}},
url = {http://aclweb.org/anthology/N18-1065},
year = {2018}
}
@inproceedings{novikova2017we,
author = {Novikova, Jekaterina and Du{\v{s}}ek, Ond{\v{r}}ej and Curry, Amanda Cercas and Rieser, Verena},
booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
mendeley-groups = {Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {2241--2252},
title = {{Why We Need New Evaluation Metrics for {\{}NLG{\}}}},
year = {2017}
}
@inproceedings{gehrmann2018bottom,
address = {Brussels, Belgium},
author = {Gehrmann, Sebastian and Deng, Yuntian and Rush, Alexander},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gehrmann, Deng, Rush - Unknown - Bottom-Up Abstractive Summarization.pdf:pdf},
mendeley-groups = {Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {4098--4109},
title = {{Bottom-Up Abstractive Summarization}},
year = {2018}
}
@inproceedings{li2018guiding,
author = {Li, Chenliang and Xu, Weiran and Li, Si and Gao, Sheng},
booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
file = {:home/acp16hh/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - Unknown - Guiding Generation for Abstractive Text Summarization based on Key Information Guide Network.pdf:pdf},
mendeley-groups = {Report and Paper,Report and Paper/ACL 2019},
pages = {55--60},
title = {{Guiding Generation for Abstractive Text Summarization Based on Key Information Guide Network}},
volume = {2},
year = {2018}
}
@inproceedings{zhang2018neural,
address = {Brussels, Belgium},
author = {Zhang, Xingxing and Lapata, Mirella and Wei, Furu and Zhou, Ming},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
mendeley-groups = {Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {779--784},
title = {{Neural Latent Extractive Document Summarization}},
year = {2018}
}
@article{Thurstone1994,
author = {Thurstone, L L},
journal = {Psychological review},
mendeley-groups = {Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
number = {2},
pages = {255--270},
title = {{A Law of Comparative Judgment}},
volume = {101},
year = {1994}
}
@inproceedings{vlachos-riedel:2014:W14-25,
address = {Baltimore, MD, USA},
author = {Vlachos, Andreas and Riedel, Sebastian},
booktitle = {Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science},
mendeley-groups = {Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
month = {jun},
pages = {18--22},
publisher = {Association for Computational Linguistics},
title = {{Fact Checking: Task definition and dataset construction}},
url = {http://www.aclweb.org/anthology/W14-2508},
year = {2014}
}
@inproceedings{fomicheva2016reference,
author = {Fomicheva, Marina and Specia, Lucia},
booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
mendeley-groups = {Report and Paper,Report and Paper/ACL 2019 (old),Report and Paper/ACL 2019},
pages = {77--82},
title = {{Reference bias in monolingual machine translation evaluation}},
volume = {2},
year = {2016}
}
@article{sokal1995biometry,
author = {Sokal, R R and Rohlf, F J},
mendeley-groups = {Report and Paper/ACL 2019},
publisher = {WH Freeman and Company},
title = {{Biometry. 3rd ed}},
year = {1995}
}
@inproceedings{gehrmann2018bottom,
address = {Brussels, Belgium},
author = {Gehrmann, Sebastian and Deng, Yuntian and Rush, Alexander},
booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
mendeley-groups = {Report and Paper/ACL 2019},
pages = {4098--4109},
title = {{Bottom-Up Abstractive Summarization}},
year = {2018}
}
